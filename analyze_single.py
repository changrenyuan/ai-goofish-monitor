#!/usr/bin/env python
"""
å•ä¸ªé—²é±¼å•†å“é“¾æ¥å¿«é€Ÿåˆ†æå·¥å…·
ä½¿ç”¨æ–¹æ³•: python analyze_single.py <å•†å“é“¾æ¥> <åˆ†ææ ‡å‡†æ–‡ä»¶(å¯é€‰)>
ç¤ºä¾‹: python analyze_single.py https://www.goofish.com/item/xxx prompts/macbook_criteria.txt
"""
import asyncio
import sys
import os
import json
import argparse
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

from src.config import (
    STATE_FILE,
    DETAIL_API_URL_PATTERN,
    RUN_HEADLESS,
    LOGIN_IS_EDGE,
    IMAGE_SAVE_DIR,
    AI_DEBUG_MODE,
)
from src.utils import (
    safe_get,
    random_sleep,
    log_time,
    get_link_unique_key,
)
from src.ai_handler import (
    download_all_images,
    get_ai_analysis,
    cleanup_task_images,
)
from src.scraper import scrape_user_profile
from src.parsers import (
    parse_user_head_data,
    _parse_user_items_data,
    parse_ratings_data,
    calculate_reputation_from_ratings,
)
from src.infrastructure.external.ai_client import AIClient


def print_section(title):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80 + "\n")


async def analyze_single_link(product_url: str, criteria_file: str = "prompts/macbook_criteria.txt"):
    """åˆ†æå•ä¸ªå•†å“é“¾æ¥"""

    # 1. éªŒè¯URLæ ¼å¼
    print_section("æ­¥éª¤ 1: éªŒè¯URLå¹¶æå–å•†å“ID")
    if "goofish.com" not in product_url:
        print("âŒ é”™è¯¯: è¯·æä¾›æœ‰æ•ˆçš„é—²é±¼å•†å“é“¾æ¥")
        print("   æ ¼å¼ç¤ºä¾‹: https://www.goofish.com/item/i123456789")
        return

    # æå–å•†å“ID
    try:
        item_id = product_url.split("/")[-1].split("?")[0]
        print(f"âœ… å•†å“ID: {item_id}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•ä»URLæå–å•†å“ID: {e}")
        return

    # 2. æ£€æŸ¥ç™»å½•çŠ¶æ€æ–‡ä»¶
    print_section("æ­¥éª¤ 2: æ£€æŸ¥ç™»å½•çŠ¶æ€")
    if not os.path.exists(STATE_FILE):
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ç™»å½•çŠ¶æ€æ–‡ä»¶: {STATE_FILE}")
        print(f"   è¯·ç¡®ä¿ state/ ç›®å½•ä¸‹æœ‰ç™»å½•çŠ¶æ€JSONæ–‡ä»¶")
        return
    print(f"âœ… ç™»å½•çŠ¶æ€æ–‡ä»¶: {STATE_FILE}")

    # 3. åŠ è½½AIåˆ†ææ ‡å‡†
    print_section("æ­¥éª¤ 3: åŠ è½½AIåˆ†ææ ‡å‡†")
    if not os.path.exists(criteria_file):
        print(f"âŒ é”™è¯¯: åˆ†ææ ‡å‡†æ–‡ä»¶ä¸å­˜åœ¨: {criteria_file}")
        print(f"   ä½¿ç”¨é»˜è®¤: prompts/macbook_criteria.txt")
        criteria_file = "prompts/macbook_criteria.txt"
        if not os.path.exists(criteria_file):
            print(f"âŒ é”™è¯¯: é»˜è®¤åˆ†ææ ‡å‡†æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨")
            return

    try:
        with open("prompts/base_prompt.txt", 'r', encoding='utf-8') as f:
            base_prompt = f.read()
        with open(criteria_file, 'r', encoding='utf-8') as f:
            criteria_text = f.read()
        ai_prompt_text = base_prompt.replace("{{CRITERIA_SECTION}}", criteria_text)
        print(f"âœ… åˆ†ææ ‡å‡†åŠ è½½æˆåŠŸ: {criteria_file}")
        print(f"   Prompté•¿åº¦: {len(ai_prompt_text)} å­—ç¬¦")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½åˆ†ææ ‡å‡†å¤±è´¥: {e}")
        return

    # 4. åˆå§‹åŒ–AIå®¢æˆ·ç«¯
    print_section("æ­¥éª¤ 4: åˆå§‹åŒ–AIå®¢æˆ·ç«¯")
    ai_client = AIClient()
    if not ai_client.is_available():
        ai_client.refresh()
    if not ai_client.is_available():
        print("âŒ é”™è¯¯: AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        print("   è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ OPENAI_API_KEY é…ç½®")
        return
    print(f"âœ… AIå®¢æˆ·ç«¯å·²å°±ç»ª")
    print(f"   æ¨¡å‹: {ai_client.settings.model_name}")
    print(f"   APIåœ°å€: {ai_client.settings.base_url}")

    # 5. å¯åŠ¨æµè§ˆå™¨å¹¶è®¿é—®å•†å“é¡µ
    print_section("æ­¥éª¤ 5: å¯åŠ¨æµè§ˆå™¨å¹¶è·å–å•†å“æ•°æ®")

    launch_args = [
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process'
    ]

    launch_kwargs = {"headless": RUN_HEADLESS, "args": launch_args}
    if LOGIN_IS_EDGE:
        launch_kwargs["channel"] = "msedge"
    else:
        launch_kwargs["channel"] = "chrome"

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(**launch_kwargs)

            # ç§»åŠ¨ç«¯ä¸Šä¸‹æ–‡
            context = await browser.new_context(
                storage_state=STATE_FILE,
                user_agent="Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Mobile Safari/537.36",
                viewport={'width': 412, 'height': 915},
                device_scale_factor=2.625,
                is_mobile=True,
                has_touch=True,
                locale='zh-CN',
                timezone_id='Asia/Shanghai',
            )

            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en-US', 'en']});
                window.chrome = {runtime: {}, loadTimes: function() {}, csi: function() {}};
                Object.defineProperty(navigator, 'maxTouchPoints', {get: () => 5});
            """)

            page = await context.new_page()

            # è®¿é—®å•†å“é¡µ
            print(f"ğŸ“± æ­£åœ¨è®¿é—®å•†å“é¡µé¢: {product_url}")
            try:
                async with page.expect_response(
                    lambda r: DETAIL_API_URL_PATTERN in r.url,
                    timeout=30000
                ) as response_info:
                    await page.goto(product_url, wait_until="domcontentloaded", timeout=60000)
                detail_response = await response_info.value

                if not detail_response.ok:
                    print(f"âŒ é”™è¯¯: è·å–å•†å“è¯¦æƒ…å¤±è´¥ï¼ŒçŠ¶æ€ç : {detail_response.status}")
                    return

                print(f"âœ… æˆåŠŸè·å–å•†å“è¯¦æƒ…APIå“åº”")

            except PlaywrightTimeoutError:
                print(f"âŒ é”™è¯¯: è®¿é—®å•†å“é¡µé¢è¶…æ—¶")
                return

            # è§£æå•†å“è¯¦æƒ…
            detail_json = await detail_response.json()
            item_do = await safe_get(detail_json, 'data', 'itemDO', default={})
            seller_do = await safe_get(detail_json, 'data', 'sellerDO', default={})

            # æ„å»ºå•†å“åŸºç¡€ä¿¡æ¯
            print_section("æ­¥éª¤ 6: è§£æå•†å“ä¿¡æ¯")

            item_data = {
                "å•†å“ID": await safe_get(item_do, 'id', default=item_id),
                "å•†å“æ ‡é¢˜": await safe_get(item_do, 'title', default="æœªçŸ¥æ ‡é¢˜"),
                "å½“å‰å”®ä»·": await safe_get(item_do, 'priceInfo', 'price', default="æœªçŸ¥ä»·æ ¼"),
                "å•†å“åŸä»·": await safe_get(item_do, 'priceInfo', 'originalPrice', default="æœªçŸ¥"),
                ""æƒ³è¦"äººæ•°": await safe_get(item_do, 'wantCnt', default='0'),
                "æµè§ˆé‡": await safe_get(item_do, 'browseCnt', default='0'),
                "å‘è´§åœ°åŒº": await safe_get(item_do, 'deliveryInfo', 'area', default="æœªçŸ¥"),
                "å•†å“é“¾æ¥": product_url,
            }

            # æå–å›¾ç‰‡åˆ—è¡¨
            image_infos = await safe_get(item_do, 'imageInfos', default=[])
            if image_infos:
                all_image_urls = [img.get('url') for img in image_infos if img.get('url')]
                item_data['å•†å“å›¾ç‰‡åˆ—è¡¨'] = all_image_urls
                item_data['å•†å“ä¸»å›¾é“¾æ¥'] = all_image_urls[0] if all_image_urls else ""
                print(f"âœ… å•†å“å›¾ç‰‡: {len(all_image_urls)} å¼ ")
            else:
                print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°å•†å“å›¾ç‰‡")

            # æ‰“å°å•†å“åŸºç¡€ä¿¡æ¯
            print(f"ğŸ“¦ å•†å“æ ‡é¢˜: {item_data['å•†å“æ ‡é¢˜']}")
            print(f"ğŸ’° å½“å‰å”®ä»·: {item_data['å½“å‰å”®ä»·']}")
            print(f"ğŸ¯ æƒ³è¦äººæ•°: {item_data['"æƒ³è¦"äººæ•°']}")
            print(f"ğŸ‘€ æµè§ˆé‡: {item_data['æµè§ˆé‡']}")
            print(f"ğŸ“ å‘è´§åœ°åŒº: {item_data['å‘è´§åœ°åŒº']}")

            # é‡‡é›†å–å®¶ä¿¡æ¯
            print_section("æ­¥éª¤ 7: é‡‡é›†å–å®¶ä¿¡æ¯")

            seller_id = await safe_get(seller_do, 'sellerId')
            if seller_id:
                print(f"ğŸ‘¤ å–å®¶ID: {seller_id}")
                print("ğŸ”„ æ­£åœ¨é‡‡é›†å–å®¶å®Œæ•´ä¿¡æ¯...")
                user_profile_data = await scrape_user_profile(context, str(seller_id))

                # æ·»åŠ é¢å¤–çš„å–å®¶ä¿¡æ¯
                zhima_credit_text = await safe_get(seller_do, 'zhimaLevelInfo', 'levelName')
                user_profile_data['å–å®¶èŠéº»ä¿¡ç”¨'] = zhima_credit_text

                print(f"âœ… å–å®¶ä¿¡æ¯é‡‡é›†å®Œæˆ")
                print(f"   å–å®¶æ˜µç§°: {user_profile_data.get('å–å®¶æ˜µç§°', 'æœªçŸ¥')}")
                print(f"   å–å®¶ä¿¡ç”¨: {user_profile_data.get('å–å®¶ä¿¡ç”¨ç­‰çº§', 'æœªçŸ¥')}")
                print(f"   åœ¨å”®å•†å“: {user_profile_data.get('å–å®¶åœ¨å”®/å·²å”®å•†å“æ•°', 'æœªçŸ¥')}")

                # ç»Ÿè®¡å–å®¶è¯„ä»·
                rating_list = user_profile_data.get('å–å®¶æ”¶åˆ°çš„è¯„ä»·åˆ—è¡¨', [])
                print(f"   è¯„ä»·æ•°é‡: {len(rating_list)} æ¡")
            else:
                print("âŒ é”™è¯¯: æ— æ³•è·å–å–å®¶ID")
                user_profile_data = {}

            # ä¸‹è½½å•†å“å›¾ç‰‡
            print_section("æ­¥éª¤ 8: ä¸‹è½½å•†å“å›¾ç‰‡")

            image_urls = item_data.get('å•†å“å›¾ç‰‡åˆ—è¡¨', [])
            if not image_urls:
                print("âš ï¸  è­¦å‘Š: æ— å›¾ç‰‡å¯ä¸‹è½½ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½")
                downloaded_image_paths = []
            else:
                downloaded_image_paths = await download_all_images(
                    item_data['å•†å“ID'],
                    image_urls,
                    task_name="single_analysis"
                )
                print(f"âœ… æˆåŠŸä¸‹è½½ {len(downloaded_image_paths)} å¼ å›¾ç‰‡")

            # æ„å»ºå®Œæ•´æ•°æ®
            final_record = {
                "çˆ¬å–æ—¶é—´": datetime.now().isoformat(),
                "å•†å“ä¿¡æ¯": item_data,
                "å–å®¶ä¿¡æ¯": user_profile_data
            }

            # AIåˆ†æ
            print_section("æ­¥éª¤ 9: AIæ·±åº¦åˆ†æ")

            print("ğŸ¤– æ­£åœ¨è°ƒç”¨AIåˆ†æå•†å“...")
            print("   (è¿™å¯èƒ½éœ€è¦30-60ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…...)")

            ai_analysis_result = await get_ai_analysis(
                final_record,
                downloaded_image_paths,
                prompt_text=ai_prompt_text
            )

            if ai_analysis_result:
                print("âœ… AIåˆ†æå®Œæˆ!")
                final_record['ai_analysis'] = ai_analysis_result

                # æ˜¾ç¤ºåˆ†æç»“æœ
                print("\n" + "="*80)
                print("  ğŸ“Š AIåˆ†æç»“æœ")
                print("="*80 + "\n")

                is_recommended = ai_analysis_result.get('is_recommended', False)
                reason = ai_analysis_result.get('reason', 'æ— ')
                risk_tags = ai_analysis_result.get('risk_tags', [])
                criteria = ai_analysis_result.get('criteria_analysis', {})
                seller_analysis = criteria.get('seller_type', {})

                # æ¨èçŠ¶æ€
                if is_recommended:
                    print("âœ… æ˜¯å¦æ¨è: æ˜¯ âœ…")
                else:
                    print("âŒ æ˜¯å¦æ¨è: å¦ âŒ")

                # æ¨èç†ç”±
                print(f"\nğŸ’¬ æ¨èç†ç”±:")
                print(f"   {reason}")

                # é£é™©æ ‡ç­¾
                if risk_tags:
                    print(f"\nâš ï¸  é£é™©æ ‡ç­¾: {', '.join(risk_tags)}")

                # å–å®¶ç”»åƒ
                if seller_analysis:
                    print(f"\nğŸ‘¤ å–å®¶ç”»åƒ:")
                    print(f"   ç±»å‹: {seller_analysis.get('status', 'æœªçŸ¥')}")
                    persona = seller_analysis.get('persona', 'æœªçŸ¥')
                    if persona:
                        print(f"   è¯¦ç»†: {persona}")

                    comment = seller_analysis.get('comment', '')
                    if comment:
                        print(f"\n   ç»¼åˆè¯„ä»·:")
                        for line in comment.split('ã€‚'):
                            if line.strip():
                                print(f"   â€¢ {line.strip()}")

                    # è¡Œä¸ºé€»è¾‘é“¾æ€»ç»“
                    details = seller_analysis.get('analysis_details', {})
                    summary = details.get('behavioral_summary', {})
                    if summary:
                        print(f"\n   è¡Œä¸ºé€»è¾‘é“¾:")
                        print(f"   {summary.get('comment', '')}")
                        evidence = summary.get('evidence', '')
                        if evidence:
                            print(f"   è¯æ®: {evidence}")

                # è¯¦ç»†åˆ†æ
                print(f"\nğŸ“‹ è¯¦ç»†åˆ†æ:")
                for key, value in criteria.items():
                    if key != 'seller_type':
                        status = value.get('status', 'æœªçŸ¥')
                        comment = value.get('comment', '')
                        if comment and status != 'PASS':
                            print(f"\n   {key}:")
                            print(f"     çŠ¶æ€: {status}")
                            print(f"     è¯´æ˜: {comment}")

                # å†³ç­–å»ºè®®
                print("\n" + "="*80)
                if is_recommended:
                    print("  ğŸ‰ å†³ç­–å»ºè®®: æ¨èè´­ä¹°!")
                else:
                    print("  âš ï¸  å†³ç­–å»ºè®®: ä¸å»ºè®®è´­ä¹°ï¼Œè¯·è°¨æ…!")
                print("="*80 + "\n")

            else:
                print("âŒ AIåˆ†æå¤±è´¥")
                final_record['ai_analysis'] = {'error': 'AI analysis failed'}

            # ä¿å­˜ç»“æœ
            print_section("æ­¥éª¤ 10: ä¿å­˜åˆ†æç»“æœ")

            output_dir = "single_analysis_results"
            os.makedirs(output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(output_dir, f"analysis_{timestamp}.json")

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_record, f, ensure_ascii=False, indent=2)

            print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # æ¸…ç†å›¾ç‰‡
            print_section("æ­¥éª¤ 11: æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
            cleanup_task_images("single_analysis")
            print("âœ… ä¸´æ—¶å›¾ç‰‡å·²æ¸…ç†")

            print_section("åˆ†æå®Œæˆ!")
            print(f"å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³: {output_file}")
            print("æ‚¨å¯ä»¥ç”¨ä»»ä½•JSONæŸ¥çœ‹å™¨æŸ¥çœ‹è¯¦ç»†æ•°æ®\n")

            await browser.close()

    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


async def main():
    parser = argparse.ArgumentParser(
        description="å•ä¸ªé—²é±¼å•†å“é“¾æ¥å¿«é€Ÿåˆ†æå·¥å…·",
        epilog="""
ç¤ºä¾‹:
  # åˆ†æMacBookå•†å“
  python analyze_single.py https://www.goofish.com/item/i123456 prompts/macbook_criteria.txt

  # åˆ†æå…¶ä»–å•†å“ï¼ˆä½¿ç”¨MacBookæ ‡å‡†ï¼‰
  python analyze_single.py https://www.goofish.com/item/i789012
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("url", help="é—²é±¼å•†å“é“¾æ¥")
    parser.add_argument("criteria", nargs='?', default="prompts/macbook_criteria.txt",
                       help="åˆ†ææ ‡å‡†æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: prompts/macbook_criteria.txtï¼‰")
    args = parser.parse_args()

    await analyze_single_link(args.url, args.criteria)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
